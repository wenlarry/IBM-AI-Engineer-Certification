<!DOCTYPE html>
<!-- saved from url=(0663)https://render.githubusercontent.com/view/ipynb?commit=bf1eb12b035d5fa9d6b9fe484211c3f46a8246f8&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4b6f6e75546563682f4275696c64696e672d446565702d4c6561726e696e672d4d6f64656c732d776974682d54656e736f72466c6f772f626631656231326230333564356661396436623966653438343231316333663436613832343666382f4d4c30313230454e2d332e322d5265766965772d4c53544d2d4c616e67756167654d6f64656c6c696e672e6970796e62&nwo=KonuTech%2FBuilding-Deep-Learning-Models-with-TensorFlow&path=ML0120EN-3.2-Review-LSTM-LanguageModelling.ipynb&repository_id=233910049&repository_type=Repository#59f7f868-e998-48ea-aa8f-21b85d5b4f16 -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Render</title>
  <meta name="referrer" content="never">
    <script src="./index-d3b2fc278f6ffca84fcdcb4b77e326d5a2a8b5ec6659efbca95dd486c43088fe.js" type="text/javascript"></script><link rel="stylesheet" type="text/css" href="./ipynb-c595452879a45fd8a1e3577c3900da53d06da195d9570ea394b693875c5e6759.css">


</head>
<body class="" data-render-url="https://render.githubusercontent.com" data-github-hostname="github.com">
  <div class="render-shell js-render-shell" data-document-nwo="KonuTech/Building-Deep-Learning-Models-with-TensorFlow" data-document-commit="bf1eb12b035d5fa9d6b9fe484211c3f46a8246f8" data-document-path="ML0120EN-3.2-Review-LSTM-LanguageModelling.ipynb" data-file="https://github-render.s3.amazonaws.com/prod/5c53bec0f5dad85168a90587ca24f366-render.html?X-Amz-Expires=65&amp;X-Amz-Date=20200812T081755Z&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJILR36AMCOMBK3MQ%2F20200812%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=64446d7c6cee688ef9ee8ace17c8df58aef1a4a5bbff98db4d0f94829ec4bd74" data-meta="https://github-render.s3.amazonaws.com/prod/5c53bec0f5dad85168a90587ca24f366-meta.json?X-Amz-Expires=65&amp;X-Amz-Date=20200812T081755Z&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJILR36AMCOMBK3MQ%2F20200812%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=47fc6fe9a2e3e085d5a6ab842f1c511fa00b0b57b1ad5a865a87baf32725804d" data-no-external-render="false">
    

<div class="render-info">
  <div class="js-viewer-health render-health is-viewer-good">
    <span class="symbol">⊖</span>
    <span class="js-message message">Everything running smoothly!</span>
  </div>
</div>

<div id="notebook" class="js-html">
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://www.bigdatauniversity.com/"><img src="./68747470733a2f2f69626d2e626f782e636f6d2f7368617265642f7374617469632f716f3230623838763168626a7a7475627430363630396f7673383571386661752e706e67" width="400px" align="center" data-canonical-src="https://ibm.box.com/shared/static/qo20b88v1hbjztubt06609ovs85q8fau.png"></a></p>
<h1 align="center"></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<h2>Applying Recurrent Neural Networks/LSTM for Language Modeling</h2>
Hello and welcome to this part. In this notebook, we will go over the topic of Language Modelling, and create a Recurrent Neural Network model based on the Long Short-Term Memory unit to train and benchmark on the Penn Treebank dataset. By the end of this notebook, you should be able to understand how TensorFlow builds and executes a RNN model for Language Modelling.

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<h2>The Objective</h2>
By now, you should have an understanding of how Recurrent Networks work -- a specialized model to process sequential data by keeping track of the "state" or context. In this notebook, we go over a TensorFlow code snippet for creating a model focused on <b>Language Modelling</b> -- a very relevant task that is the cornerstone of many different linguistic problems such as <b>Speech Recognition, Machine Translation and Image Captioning</b>. For this, we will be using the Penn Treebank dataset, which is an often-used dataset for benchmarking Language Modelling models.

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<h2>Table of Contents</h2>
<p></p>
<ol>
    <li><a href="https://render.githubusercontent.com/view/ipynb?commit=bf1eb12b035d5fa9d6b9fe484211c3f46a8246f8&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4b6f6e75546563682f4275696c64696e672d446565702d4c6561726e696e672d4d6f64656c732d776974682d54656e736f72466c6f772f626631656231326230333564356661396436623966653438343231316333663436613832343666382f4d4c30313230454e2d332e322d5265766965772d4c53544d2d4c616e67756167654d6f64656c6c696e672e6970796e62&amp;nwo=KonuTech%2FBuilding-Deep-Learning-Models-with-TensorFlow&amp;path=ML0120EN-3.2-Review-LSTM-LanguageModelling.ipynb&amp;repository_id=233910049&amp;repository_type=Repository#language_modelling">What exactly is Language Modelling?</a></li>
    <li><a href="https://render.githubusercontent.com/view/ipynb?commit=bf1eb12b035d5fa9d6b9fe484211c3f46a8246f8&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4b6f6e75546563682f4275696c64696e672d446565702d4c6561726e696e672d4d6f64656c732d776974682d54656e736f72466c6f772f626631656231326230333564356661396436623966653438343231316333663436613832343666382f4d4c30313230454e2d332e322d5265766965772d4c53544d2d4c616e67756167654d6f64656c6c696e672e6970796e62&amp;nwo=KonuTech%2FBuilding-Deep-Learning-Models-with-TensorFlow&amp;path=ML0120EN-3.2-Review-LSTM-LanguageModelling.ipynb&amp;repository_id=233910049&amp;repository_type=Repository#treebank_dataset">The Penn Treebank dataset</a></li>
    <li><a href="https://render.githubusercontent.com/view/ipynb?commit=bf1eb12b035d5fa9d6b9fe484211c3f46a8246f8&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4b6f6e75546563682f4275696c64696e672d446565702d4c6561726e696e672d4d6f64656c732d776974682d54656e736f72466c6f772f626631656231326230333564356661396436623966653438343231316333663436613832343666382f4d4c30313230454e2d332e322d5265766965772d4c53544d2d4c616e67756167654d6f64656c6c696e672e6970796e62&amp;nwo=KonuTech%2FBuilding-Deep-Learning-Models-with-TensorFlow&amp;path=ML0120EN-3.2-Review-LSTM-LanguageModelling.ipynb&amp;repository_id=233910049&amp;repository_type=Repository#word_embedding">Work Embedding</a></li>
    <li><a href="https://render.githubusercontent.com/view/ipynb?commit=bf1eb12b035d5fa9d6b9fe484211c3f46a8246f8&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4b6f6e75546563682f4275696c64696e672d446565702d4c6561726e696e672d4d6f64656c732d776974682d54656e736f72466c6f772f626631656231326230333564356661396436623966653438343231316333663436613832343666382f4d4c30313230454e2d332e322d5265766965772d4c53544d2d4c616e67756167654d6f64656c6c696e672e6970796e62&amp;nwo=KonuTech%2FBuilding-Deep-Learning-Models-with-TensorFlow&amp;path=ML0120EN-3.2-Review-LSTM-LanguageModelling.ipynb&amp;repository_id=233910049&amp;repository_type=Repository#building_lstm_model">Building the LSTM model for Language Modeling</a></li>
    <li><a href="https://render.githubusercontent.com/view/ipynb?commit=bf1eb12b035d5fa9d6b9fe484211c3f46a8246f8&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4b6f6e75546563682f4275696c64696e672d446565702d4c6561726e696e672d4d6f64656c732d776974682d54656e736f72466c6f772f626631656231326230333564356661396436623966653438343231316333663436613832343666382f4d4c30313230454e2d332e322d5265766965772d4c53544d2d4c616e67756167654d6f64656c6c696e672e6970796e62&amp;nwo=KonuTech%2FBuilding-Deep-Learning-Models-with-TensorFlow&amp;path=ML0120EN-3.2-Review-LSTM-LanguageModelling.ipynb&amp;repository_id=233910049&amp;repository_type=Repository#ltsm">LTSM</a></li>
</ol>
<p></p>
<p></p>
&lt;/div&gt;
<br>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a></a></p>
<p></p>
<h2>What exactly is Language Modelling?</h2>
Language Modelling, to put it simply, <b>is the task of assigning probabilities to sequences of words</b>. This means that, given a context of one or a sequence of words in the language the model was trained on, the model should provide the next most probable words or sequence of words that follows from the given sequence of words the sentence. Language Modelling is one of the most important tasks in Natural Language Processing.
<p><img src="./68747470733a2f2f69626d2e626f782e636f6d2f7368617265642f7374617469632f316431693567756236776c6a62793276616e6932767a787030787370683730322e706e67" width="1080" data-canonical-src="https://ibm.box.com/shared/static/1d1i5gub6wljby2vani2vzxp0xsph702.png"></p>
<p></p>
<br><br>
In this example, one can see the predictions for the next word of a sentence, given the context "This is an". As you can see, this boils down to a sequential data analysis task -- you are given a word or a sequence of words (the input data), and, given the context (the state), you need to find out what is the next word (the prediction). This kind of analysis is very important for language-related tasks such as <b>Speech Recognition, Machine Translation, Image Captioning, Text Correction</b> and many other very relevant problems.
<p><img src="./68747470733a2f2f69626d2e626f782e636f6d2f7368617265642f7374617469632f617a33396964663969706664706335756769667067786e7964656c68796633692e706e67" width="1080" data-canonical-src="https://ibm.box.com/shared/static/az39idf9ipfdpc5ugifpgxnydelhyf3i.png"></p>
<p></p>
<br><br>
As the above image shows, Recurrent Network models fit this problem like a glove. Alongside LSTM and its capacity to maintain the model's state for over one thousand time steps, we have all the tools we need to undertake this problem. The goal for this notebook is to create a model that can reach <b>low levels of perplexity</b> on our desired dataset.
<p>For Language Modelling problems, <b>perplexity</b> is the way to gauge efficiency. Perplexity is simply a measure of how well a probabilistic model is able to predict its sample. A higher-level way to explain this would be saying that <b>low perplexity means a higher degree of trust in the predictions the model makes</b>. Therefore, the lower perplexity is, the better.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a></a></p>
<p></p>
<h2>The Penn Treebank dataset</h2>
Historically, datasets big enough for Natural Language Processing are hard to come by. This is in part due to the necessity of the sentences to be broken down and tagged with a certain degree of correctness -- or else the models trained on it won't be able to be correct at all. This means that we need a <b>large amount of data, annotated by or at least corrected by humans</b>. This is, of course, not an easy task at all.
<p>The Penn Treebank, or PTB for short, is a dataset maintained by the University of Pennsylvania. It is <i>huge</i> -- there are over <b>four million and eight hundred thousand</b> annotated words in it, all corrected by humans. It is composed of many different sources, from abstracts of Department of Energy papers to texts from the Library of America. Since it is verifiably correct and of such a huge size, the Penn Treebank is commonly used as a benchmark dataset for Language Modelling.</p>
<p>The dataset is divided in different kinds of annotations, such as Piece-of-Speech, Syntactic and Semantic skeletons. For this example, we will simply use a sample of clean, non-annotated words (with the exception of one tag --<code>&lt;unk&gt;</code>
, which is used for rare words such as uncommon proper nouns) for our model. This means that we just want to predict what the next words would be, not what they mean in context or their classes on a given sentence.</p>
<p></p>
<br><br>
<div class="alert alert-block alert-info">
    
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a></a></p>
<p></p>
<h2>Word Embeddings</h2>
<br>
<p>For better processing, in this example, we will make use of <a href="https://www.tensorflow.org/tutorials/word2vec/"><b>word embeddings</b></a>, which is <b>a way of representing sentence structures or words as n-dimensional vectors (where n is a reasonably high number, such as 200 or 500) of real numbers</b>. Basically, we will assign each word a randomly-initialized vector, and input those into the network to be processed. After a number of iterations, these vectors are expected to assume values that help the network to correctly predict what it needs to -- in our case, the probable next word in the sentence. This is shown to be a very effective task in Natural Language Processing, and is a commonplace practice.
<br><br>

<br>
Word Embedding tends to group up similarly used words <i>reasonably</i> close together in the vectorial space. For example, if we use T-SNE (a dimensional reduction visualization algorithm) to flatten the dimensions of our vectors into a 2-dimensional space and plot these words in a 2-dimensional space, we might see something like this:</p>
<p><img src="./68747470733a2f2f69626d2e626f782e636f6d2f7368617265642f7374617469632f6271686335646738373967636f61627a68787261317738726b67336f643163752e706e67" width="800" data-canonical-src="https://ibm.box.com/shared/static/bqhc5dg879gcoabzhxra1w8rkg3od1cu.png"></p>
<p></p>
<br><br>
As you can see, words that are frequently used together, in place of each other, or in the same places as them tend to be grouped together -- being closer together the higher they are correlated. For example, "None" is pretty semantically close to "Zero", while a phrase that uses "Italy", you could probably also fit "Germany" in it, with little damage to the sentence structure. The vectorial "closeness" for similar words like this is a great indicator of a well-built model.
<hr>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We need to import the necessary modules for our code. We need <b><code>numpy</code></b> and <b><code>tensorflow</code></b>, obviously. Additionally, we can import directly the <b><code>tensorflow.models.rnn</code></b> model, which includes the function for building RNNs, and <b><code>tensorflow.models.rnn.ptb.reader</code></b> which is the helper module for getting the input data from the dataset we just downloaded.</p>
<p>If you want to learn more take a look at <a href="https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py">https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">reader</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="err">!</span><span class="n">mkdir</span> <span class="n">data</span>
<span class="err">!</span><span class="n">wget</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">O</span> <span class="n">data</span><span class="o">/</span><span class="n">ptb</span><span class="o">.</span><span class="n">zip</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">ibm</span><span class="o">.</span><span class="n">box</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">shared</span><span class="o">/</span><span class="n">static</span><span class="o">/</span><span class="n">z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r</span><span class="o">.</span><span class="n">zip</span>
<span class="err">!</span><span class="n">unzip</span> <span class="o">-</span><span class="n">o</span> <span class="n">data</span><span class="o">/</span><span class="n">ptb</span><span class="o">.</span><span class="n">zip</span> <span class="o">-</span><span class="n">d</span> <span class="n">data</span>
<span class="err">!</span><span class="n">cp</span> <span class="n">data</span><span class="o">/</span><span class="n">ptb</span><span class="o">/</span><span class="n">reader</span><span class="o">.</span><span class="n">py</span> <span class="o">.</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>mkdir: cannot create directory ‘data’: File exists
Archive:  data/ptb.zip
  inflating: data/ptb/reader.py      
  inflating: data/__MACOSX/ptb/._reader.py  
  inflating: data/__MACOSX/._ptb     
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a></a></p>
<p></p>
<h2>Building the LSTM model for Language Modeling</h2>
Now that we know exactly what we are doing, we can start building our model using TensorFlow. The very first thing we need to do is download and extract the <code>simple-examples</code> dataset, which can be done by executing the code cell below.

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="err">!</span><span class="n">wget</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">fit</span><span class="o">.</span><span class="n">vutbr</span><span class="o">.</span><span class="n">cz</span><span class="o">/~</span><span class="n">imikolov</span><span class="o">/</span><span class="n">rnnlm</span><span class="o">/</span><span class="n">simple</span><span class="o">-</span><span class="n">examples</span><span class="o">.</span><span class="n">tgz</span> 
<span class="err">!</span><span class="n">tar</span> <span class="n">xzf</span> <span class="n">simple</span><span class="o">-</span><span class="n">examples</span><span class="o">.</span><span class="n">tgz</span> <span class="o">-</span><span class="n">C</span> <span class="n">data</span><span class="o">/</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>--2020-02-02 11:28:39--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz
Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917
Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 34869662 (33M) [application/x-gtar]
Saving to: ‘simple-examples.tgz.1’

simple-examples.tgz 100%[===================&gt;]  33.25M  4.10MB/s    in 9.3s    

2020-02-02 11:28:49 (3.56 MB/s) - ‘simple-examples.tgz.1’ saved [34869662/34869662]

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Additionally, for the sake of making it easy to play around with the model's hyperparameters, we can declare them beforehand. Feel free to change these -- you will see a difference in performance each time you change those!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Initial weight scale</span>
<span class="n">init_scale</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="c1">#Initial learning rate</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="c1">#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)</span>
<span class="n">max_grad_norm</span> <span class="o">=</span> <span class="mi">5</span>
<span class="c1">#The number of layers in our model</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1">#The total number of recurrence steps, also known as the number of layers when our RNN is "unfolded"</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">20</span>
<span class="c1">#The number of processing units (neurons) in the hidden layers</span>
<span class="n">hidden_size_l1</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">hidden_size_l2</span> <span class="o">=</span> <span class="mi">128</span>
<span class="c1">#The maximum number of epochs trained with the initial learning rate</span>
<span class="n">max_epoch_decay_lr</span> <span class="o">=</span> <span class="mi">4</span>
<span class="c1">#The total number of epochs in training</span>
<span class="n">max_epoch</span> <span class="o">=</span> <span class="mi">15</span>
<span class="c1">#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)</span>
<span class="c1">#At 1, we ignore the Dropout Layer wrapping.</span>
<span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1">#The decay for the learning rate</span>
<span class="n">decay</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="c1">#The size for each batch of data</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">60</span>
<span class="c1">#The size of our vocabulary</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">embeding_vector_size</span> <span class="o">=</span> <span class="mi">200</span>
<span class="c1">#Training flag to separate training from testing</span>
<span class="n">is_training</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1">#Data directory for our dataset</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s2">"data/simple-examples/data/"</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Some clarifications for LSTM architecture based on the arguments:</p>
<p>Network structure:</p>
<p></p>
<ul>
    <li>In this network, the number of LSTM cells are 2. To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.
    </li>
    <li>The recurrence steps is 20, that is, when our RNN is "Unfolded", the recurrence step is 20.</li>   
    <li>the structure is like:
        <ul>
            <li>200 input units -&gt; [200x200] Weight -&gt; 200 Hidden units (first layer) -&gt; [200x200] Weight matrix  -&gt; 200 Hidden units (second layer) -&gt;  [200] weight Matrix -&gt; 200 unit output</li>
        </ul>
    &lt;/li&gt;
&lt;/ul&gt;
<br>
<p>Input layer:</p>
<p></p>
<ul>
    <li>The network has 200 input units.</li>
    <li>Suppose each word is represented by an embedding vector of dimensionality e=200. The input layer of each cell will have 200 linear units. These e=200 linear units are connected to each of the h=200 LSTM units in the hidden layer (assuming there is only one hidden layer, though our case has 2 layers).
    </li>
    <li>The input shape is [batch_size, num_steps], that is [30x20]. It will turn into [30x20x200] after embedding, and then 20x[30x200]
    </li>
</ul>
<br>
<p>Hidden layer:</p>
<p></p>
<ul>
    <li>Each LSTM has 200 hidden units which is equivalent to the dimensionality of the embedding words and output.</li>
</ul>
<br>

</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is a lot to be done and a ton of information to process at the same time, so go over this code slowly. It may seem complex at first, but if you try to apply what you just learned about language modelling to the code you see, you should be able to understand it.</p>
<p>This code is adapted from the <a href="https://github.com/tensorflow/models">PTBModel</a> example bundled with the TensorFlow source code.</p>
<p></p>
<h4>Train data</h4>
The story starts from data:
<ul>
    <li>Train data is a list of words, of size 929589, represented by numbers, e.g. [9971, 9972, 9974, 9975,...]</li>
    <li>We read data as mini-batch of size b=30. Assume the size of each sentence is 20 words (num_steps = 20). Then it will take <img class="math math-display" alt="$$floor(\frac{N}{b \times h})+1=1548$$" src="./math"> iterations for the learner to go through all sentences once. Where N is the size of the list of words, b is batch size, andh is size of each sentence. So, the number of iterators is 1548
    </li>
    <li>Each batch data is read from train dataset of size 600, and shape of [30x20]</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we start an interactive session:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">InteractiveSession</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).
  warnings.warn('An interactive session is already active. This can '
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Reads the data and separates it into training data, validation data and testing data</span>
<span class="n">raw_data</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">ptb_raw_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">valid_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">word_to_id</span> <span class="o">=</span> <span class="n">raw_data</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[11]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>929589</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">id_to_word</span><span class="p">(</span><span class="n">id_list</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">id_list</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">wid</span> <span class="ow">in</span> <span class="n">word_to_id</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">wid</span> <span class="o">==</span> <span class="n">w</span><span class="p">:</span>
                <span class="n">line</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">line</span>            
                

<span class="nb">print</span><span class="p">(</span><span class="n">id_to_word</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '&lt;eos&gt;', 'pierre', '&lt;unk&gt;', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '&lt;eos&gt;', 'mr.', '&lt;unk&gt;', 'is', 'chairman', 'of', '&lt;unk&gt;', 'n.v.', 'the', 'dutch', 'publishing', 'group', '&lt;eos&gt;', 'rudolph', '&lt;unk&gt;', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '&lt;eos&gt;', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets just read one mini-batch now and feed our network:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">itera</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">ptb_iterator</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>
<span class="n">first_touple</span> <span class="o">=</span> <span class="n">itera</span><span class="o">.</span><span class="n">__next__</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">first_touple</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">first_touple</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[14]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>(60, 20)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets look at 3 sentences of our input x:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[15]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,
        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],
       [ 901,   33, 3361,    8, 1279,  437,  597,    6,  261, 4276, 1089,
           8, 2836,    2,  269,    4, 5526,  241,   13, 2420],
       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,
         123,    7,  514,    2,   63,   10,  514,    8,  605]],
      dtype=int32)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>we define 2 place holders to feed them with mini-batchs, that is x and y:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_input_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">])</span> <span class="c1">#[30#20]</span>
<span class="n">_targets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">])</span> <span class="c1">#[30#20]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets define a dictionary, and use it later to feed the placeholders with our first mini-batch:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">_input_data</span><span class="p">:</span><span class="n">x</span><span class="p">,</span> <span class="n">_targets</span><span class="p">:</span><span class="n">y</span><span class="p">}</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For example, we can use it to feed <code>\_input\_data</code>:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">_input_data</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[18]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>array([[9970, 9971, 9972, ..., 9993, 9994, 9995],
       [ 901,   33, 3361, ...,  241,   13, 2420],
       [2654,    6,  334, ...,  514,    8,  605],
       ...,
       [7831,   36, 1678, ...,    4, 4558,  157],
       [  59, 2070, 2433, ...,  400,    1, 1173],
       [2097,    3,    2, ..., 2043,   23,    1]], dtype=int32)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this step, we create the stacked LSTM, which is a 2 layer LSTM network:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lstm_cell_l1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">hidden_size_l1</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">lstm_cell_l2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">hidden_size_l2</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">stacked_lstm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">lstm_cell_l1</span><span class="p">,</span> <span class="n">lstm_cell_l2</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also, we initialize the states of the nework:</p>
<h4>_initial_state</h4>
<p>For each LCTM, there are 2 state matrices, c_state and m_state.  c_state and m_state represent "Memory State" and "Cell State". Each hidden layer, has a vector of size 30, which keeps the states. so, for 200 hidden units in each LSTM, we have a matrix of size [30x200]</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_initial_state</span> <span class="o">=</span> <span class="n">stacked_lstm</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">_initial_state</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[20]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>(LSTMStateTuple(c=&lt;tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros:0' shape=(60, 256) dtype=float32&gt;, h=&lt;tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(60, 256) dtype=float32&gt;),
 LSTMStateTuple(c=&lt;tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros:0' shape=(60, 128) dtype=float32&gt;, h=&lt;tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros_1:0' shape=(60, 128) dtype=float32&gt;))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets look at the states, though they are all zero for now:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">_initial_state</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[21]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>(LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)),
 LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<h3>Embeddings</h3>
We have to convert the words in our dataset to vectors of numbers. The traditional approach is to use one-hot encoding method that is usually used for converting categorical values to numerical values. However, One-hot encoded vectors are high-dimensional, sparse and in a big dataset, computationally inefficient. So, we use word2vec approach. It is, in fact, a layer in our LSTM network, where the word IDs will be represented as a dense representation before feeding to the LSTM.
<p>The embedded vectors also get updated during the training process of the deep neural network.
We create the embeddings for our input data. <b>embedding_vocab</b> is matrix of [10000x200] for all 10000 unique words.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embedding_vocab</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">"embedding_vocab"</span><span class="p">,</span> <span class="p">[</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embeding_vector_size</span><span class="p">])</span>  <span class="c1">#[10000x200]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets initialize the <code>embedding_words</code> variable with random values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">embedding_vocab</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[23]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>array([[-0.01415229,  0.0020615 ,  0.00483348, ..., -0.02174182,
        -0.01958222, -0.0019541 ],
       [ 0.01009297, -0.0221437 ,  0.01230779, ..., -0.00450121,
         0.02269965, -0.01104087],
       [ 0.00581276, -0.00230083,  0.0093035 , ..., -0.00537598,
        -0.00462674,  0.00240099],
       ...,
       [-0.02328045, -0.01938979, -0.02192219, ...,  0.00431917,
        -0.00011383, -0.00199659],
       [ 0.00269953, -0.02154553,  0.01704296, ..., -0.00894606,
        -0.0072019 , -0.00217836],
       [ 0.00376112,  0.00271794, -0.009487  , ...,  0.00215217,
        -0.01371387,  0.01537636]], dtype=float32)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><b>embedding_lookup()</b> finds the embedded values for our batch of 30x20 words. It  goes to each row of <code>input_data</code>, and for each word in the row/sentence, finds the correspond vector in <code>embedding_dic<code>. <br>
It creates a [30x20x200] tensor, so, the first element of <b>inputs</b> (the first sentence), is a matrix of 20x200, which each row of it, is vector representing a word in the sentence.</code></code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define where to get the data for our embeddings from</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding_vocab</span><span class="p">,</span> <span class="n">_input_data</span><span class="p">)</span>  <span class="c1">#shape=(30, 20, 200) </span>
<span class="n">inputs</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[24]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>&lt;tf.Tensor 'embedding_lookup:0' shape=(60, 20, 200) dtype=float32&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[25]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>array([[-0.0064902 , -0.01207784, -0.00260266, ...,  0.0206253 ,
        -0.00664146,  0.01388235],
       [ 0.00208091, -0.01093761, -0.02060855, ..., -0.0176512 ,
         0.01555531,  0.02231719],
       [ 0.00229493,  0.00974536,  0.00411461, ..., -0.02234322,
         0.00752835, -0.01729599],
       ...,
       [-0.00346862, -0.01429997,  0.01049082, ..., -0.01083067,
         0.01560483, -0.004133  ],
       [-0.01236486,  0.01851235, -0.00048184, ..., -0.0117697 ,
         0.01201736,  0.01487473],
       [-0.01379531, -0.00858414,  0.00865652, ..., -0.01257758,
         0.00151328, -0.00916983]], dtype=float32)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<h3>Constructing Recurrent Neural Networks</h3>
<b>tf.nn.dynamic_rnn()</b> creates a recurrent neural network using <b>stacked_lstm</b>.
<p>The input should be a Tensor of shape: [batch_size, max_time, embedding_vector_size], in our case it would be (30, 20, 200)</p>
<p>This method, returns a pair (outputs, new_state) where:</p>
<ul>
    <li>
<b>outputs</b>: is a length T list of outputs (one for each input), or a nested tuple of such elements.</li>
    <li>
<b>new_state</b>: is the final state.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">outputs</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span>  <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">stacked_lstm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">_initial_state</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>so, lets look at the outputs. The output of the stackedLSTM comes from 200 hidden_layer, and in each time step(=20), one of them get activated. we use the linear activation to map the 200 hidden layer to a [?x10 matrix]</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">outputs</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[27]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>&lt;tf.Tensor 'rnn/transpose_1:0' shape=(60, 20, 128) dtype=float32&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[28]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>array([[-1.19189201e-04, -2.21901515e-04, -2.66724441e-04, ...,
         1.79185343e-04, -1.39329859e-04, -5.54598053e-04],
       [-2.98619503e-04, -6.10693125e-04, -4.65749530e-04, ...,
        -6.71920279e-05,  2.96846847e-04, -2.63071743e-05],
       [-5.01525719e-05, -9.99751966e-04,  1.21005702e-04, ...,
        -5.49632859e-05,  6.09378854e-04,  6.72306051e-04],
       ...,
       [-4.19432763e-04, -7.15446193e-04, -1.88918988e-04, ...,
        -1.87663233e-03, -2.64889008e-04,  7.57927482e-04],
       [-7.73191336e-04, -1.10169302e-03, -1.68767765e-05, ...,
        -1.64000061e-03, -1.43042445e-04,  3.22060572e-04],
       [-4.49966057e-04, -9.23670596e-04,  1.55205416e-04, ...,
        -1.64465094e-03,  1.15167524e-04,  1.92213876e-04]], dtype=float32)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>we need to flatten the outputs to be able to connect it softmax layer. Lets reshape the output tensor from  [30 x 20 x 200] to [600 x 200].</p>
<p><b>Notice:</b> Imagine our output is 3-d tensor as following (of course each <code>sen_x_word_y</code> is a an embedded vector by itself):</p>
<p></p>
<ul>
    <li>sentence 1: [[sen1word1], [sen1word2], [sen1word3], ..., [sen1word20]]</li> 
    <li>sentence 2: [[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20]]</li>   
    <li>sentence 3: [[sen3word1], [sen3word2], [sen3word3], ..., [sen3word20]]</li>  
    <li>...  </li>
    <li>sentence 30: [[sen30word1], [sen30word2], [sen30word3], ..., [sen30word20]]</li>   
</ul>
Now, the flatten would convert this 3-dim tensor to:
<p>[ [sen1word1], [sen1word2], [sen1word3], ..., [sen1word20],[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20], ..., [sen30word20] ]</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size_l2</span><span class="p">])</span>
<span class="n">output</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[29]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>&lt;tf.Tensor 'Reshape:0' shape=(1200, 128) dtype=float32&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<h3>logistic unit</h3>
Now, we create a logistic unit to return the probability of the output word in our vocabulary with 1000 words.
<img class="math math-display" alt="$$Softmax = [600 \times 200] * [200 \times 1000] + [1 \times 1000] \Longrightarrow [600 \times 1000]$$" src="./math(1)">
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">softmax_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">"softmax_w"</span><span class="p">,</span> <span class="p">[</span><span class="n">hidden_size_l2</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">])</span> <span class="c1">#[200x1000]</span>
<span class="n">softmax_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">"softmax_b"</span><span class="p">,</span> <span class="p">[</span><span class="n">vocab_size</span><span class="p">])</span> <span class="c1">#[1x1000]</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">softmax_w</span><span class="p">)</span> <span class="o">+</span> <span class="n">softmax_b</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets look at the probability of observing words for t=0 to t=20:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[31]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">output_words_prob</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"shape of the output: "</span><span class="p">,</span> <span class="n">output_words_prob</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"The probability of observing words in t=0 to t=20"</span><span class="p">,</span> <span class="n">output_words_prob</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>shape of the output:  (1200, 10000)
The probability of observing words in t=0 to t=20 [[1.00266683e-04 1.01174359e-04 9.92664136e-05 ... 9.82952843e-05
  9.97438692e-05 1.00532918e-04]
 [1.00270976e-04 1.01170968e-04 9.92712085e-05 ... 9.83011487e-05
  9.97425086e-05 1.00526580e-04]
 [1.00268669e-04 1.01171208e-04 9.92611822e-05 ... 9.83010177e-05
  9.97433453e-05 1.00523663e-04]
 ...
 [1.00266407e-04 1.01175487e-04 9.92567730e-05 ... 9.82916899e-05
  9.97421885e-05 1.00528479e-04]
 [1.00250683e-04 1.01166479e-04 9.92604619e-05 ... 9.82965430e-05
  9.97485040e-05 1.00528669e-04]
 [1.00248260e-04 1.01167454e-04 9.92575297e-05 ... 9.82902857e-05
  9.97535753e-05 1.00524267e-04]]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<h3>Prediction</h3>
What is the word correspond to the probability output? Lets use the maximum probability:

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output_words_prob</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[32]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>array([2713, 2713, 3913, 3913, 7784, 6848, 6848, 7784,  616,  616, 5706,
       5706, 9432, 9432, 9432, 2073, 2073, 2073, 9432, 2073])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, what is the ground truth for the first word of first sentence?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[33]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,
       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also, you can get it from target tensor, if you want to find the embedding vector:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">targ</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">_targets</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span> 
<span class="n">targ</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[34]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,
       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>How similar the predicted words are to the target words?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>Objective function</h4>
<p>Now we have to define our objective function, to calculate the similarity of predicted values to ground truth, and then, penalize the model with the error. Our objective is to minimize loss function, that is, to minimize the average negative log probability of the target words:</p>
<img class="math math-display" alt="$$\text{loss} = -\frac{1}{N}\sum_{i=1}^{N} \ln p_{\text{target}_i}$$" src="./math(2)"><p>This function is already implemented and available in TensorFlow through <b>sequence_loss_by_example</b>. It calculates the weighted cross-entropy loss for <b>logits</b> and the <b>target</b> sequence.</p>
<p>The arguments of this function are:</p>
<ul>
    <li>logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</li>  
    <li>targets: List of 1D batch-sized int32 Tensors of the same length as logits.</li>   
    <li>weights: List of 1D batch-sized float-Tensors of the same length as logits.</li> 
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[35]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">legacy_seq2seq</span><span class="o">.</span><span class="n">sequence_loss_by_example</span><span class="p">([</span><span class="n">logits</span><span class="p">],</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">_targets</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])],[</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_steps</span><span class="p">])])</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>loss is a 1D batch-sized float Tensor [600x1]: The log-perplexity for each sequence. Lets look at the first 10 values of loss:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[36]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>array([9.205525 , 9.2144985, 9.221913 , 9.202395 , 9.227202 , 9.210929 ,
       9.209447 , 9.1944895, 9.210582 , 9.210322 ], dtype=float32)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, we define loss as average of the losses:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[37]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>184.25804</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3>Training</h3>
<p>To do training for our network, we have to take the following steps:</p>
<ol>
    <li>Define the optimizer.</li>
    <li>Extract variables that are trainable.</li>
    <li>Calculate the gradients based on the loss function.</li>
    <li>Apply the optimizer to the variables/gradients tuple.</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>1. Define Optimizer</h4>
<p><b>GradientDescentOptimizer</b> constructs a new gradient descent optimizer. Later, we use constructed <b>optimizer</b> to compute gradients for a loss and apply gradients to variables.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create a variable for the learning rate</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Create the gradient descent optimizer with our learning rate</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>2. Trainable Variables</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Defining a variable, if you passed <i>trainable=True</i>, the variable constructor automatically adds new variables to the graph collection <b>GraphKeys.TRAINABLE_VARIABLES</b>. Now, using <i>tf.trainable_variables()</i> you can get all variables created with <b>trainable=True</b>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Get all TensorFlow variables marked as "trainable" (i.e. all of them except _lr, which we just created)</span>
<span class="n">tvars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()</span>
<span class="n">tvars</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[39]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>[&lt;tf.Variable 'embedding_vocab:0' shape=(10000, 200) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(456, 1024) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1024,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'softmax_w:0' shape=(128, 10000) dtype=float32_ref&gt;,
 &lt;tf.Variable 'softmax_b:0' shape=(10000,) dtype=float32_ref&gt;]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: we can find the name and scope of all variables:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[40]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tvars</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[40]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>['embedding_vocab:0',
 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
 'softmax_w:0',
 'softmax_b:0']</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>3. Calculate the gradients based on the loss function</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<h4>Gradient</h4>:
The gradient of a function is the slope of its derivative (line), or in other words, the rate of change of a function. It's a vector (a direction to move) that points in the direction of greatest increase of the function, and calculated by the <b>derivative</b> operation.
<p>First lets recall the gradient function using an toy example:
<img class="math math-display" alt="$$ z = \left(2x^2 + 3xy\right)$$" src="./math(3)"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[41]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">var_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">var_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> 
<span class="n">func_test</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">var_x</span> <span class="o">*</span> <span class="n">var_x</span> <span class="o">+</span> <span class="mf">3.0</span> <span class="o">*</span> <span class="n">var_x</span> <span class="o">*</span> <span class="n">var_y</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">func_test</span><span class="p">,</span> <span class="p">{</span><span class="n">var_x</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span><span class="n">var_y</span><span class="p">:</span><span class="mf">2.0</span><span class="p">})</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[41]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>8.0</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <b>tf.gradients()</b> function allows you to compute the symbolic gradient of one tensor with respect to one or more other tensors—including variables. <b>tf.gradients(func, xs)</b> constructs symbolic partial derivatives of sum of <b>func</b> w.r.t. <i>x</i> in <b>xs</b>.</p>
<p>Now, lets look at the derivitive w.r.t. <b>var_x</b>:
<img class="math math-display" alt="$$ \frac{\partial \:}{\partial \:x}\left(2x^2 + 3xy\right) = 4x + 3y $$" src="./math(4)"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[42]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">var_grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">func_test</span><span class="p">,</span> <span class="p">[</span><span class="n">var_x</span><span class="p">])</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">var_grad</span><span class="p">,</span> <span class="p">{</span><span class="n">var_x</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span><span class="n">var_y</span><span class="p">:</span><span class="mf">2.0</span><span class="p">})</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[42]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>[10.0]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>the derivative w.r.t. <b>var_y</b>:
<img class="math math-display" alt="$$ \frac{\partial \:}{\partial \:x}\left(2x^2 + 3xy\right) = 3x $$" src="./math(5)"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[43]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">var_grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">func_test</span><span class="p">,</span> <span class="p">[</span><span class="n">var_y</span><span class="p">])</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">var_grad</span><span class="p">,</span> <span class="p">{</span><span class="n">var_x</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">var_y</span><span class="p">:</span><span class="mf">2.0</span><span class="p">})</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[43]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>[3.0]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, we can look at gradients w.r.t all variables:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[44]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">tvars</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[44]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>[&lt;tensorflow.python.framework.ops.IndexedSlices at 0x7f586c0e1d68&gt;,
 &lt;tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(456, 1024) dtype=float32&gt;,
 &lt;tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(1024,) dtype=float32&gt;,
 &lt;tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(384, 512) dtype=float32&gt;,
 &lt;tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(512,) dtype=float32&gt;,
 &lt;tf.Tensor 'gradients_2/MatMul_grad/MatMul_1:0' shape=(128, 10000) dtype=float32&gt;,
 &lt;tf.Tensor 'gradients_2/add_grad/Reshape_1:0' shape=(10000,) dtype=float32&gt;]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[45]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grad_t_list</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">tvars</span><span class="p">)</span>
<span class="c1">#sess.run(grad_t_list,feed_dict)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>now, we have a list of tensors, t-list. We can use it to find clipped tensors. <b>clip_by_global_norm</b> clips values of multiple tensors by the ratio of the sum of their norms.</p>
<p><b>clip_by_global_norm</b> get <i>t-list</i> as input and returns 2 things:</p>
<ul>
    <li>a list of clipped tensors, so called <i>list_clipped</i>
</li> 
    <li>the global norm (global_norm) of all tensors in t_list</li> 
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[46]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define the gradient clipping threshold</span>
<span class="n">grads</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">grad_t_list</span><span class="p">,</span> <span class="n">max_grad_norm</span><span class="p">)</span>
<span class="n">grads</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[46]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>[&lt;tensorflow.python.framework.ops.IndexedSlices at 0x7f586c083400&gt;,
 &lt;tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(456, 1024) dtype=float32&gt;,
 &lt;tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(1024,) dtype=float32&gt;,
 &lt;tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(384, 512) dtype=float32&gt;,
 &lt;tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(512,) dtype=float32&gt;,
 &lt;tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(128, 10000) dtype=float32&gt;,
 &lt;tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_6:0' shape=(10000,) dtype=float32&gt;]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[47]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[47]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>[IndexedSlicesValue(values=array([[-2.61569548e-05,  6.16112402e-06, -2.66044071e-06, ...,
          1.74453453e-05, -1.04860119e-05,  5.10769542e-06],
        [-1.77246911e-05,  1.21485164e-05, -1.38116206e-06, ...,
          1.11905920e-05, -5.60408034e-06,  7.72732164e-06],
        [-8.68449661e-06,  1.11302970e-05,  6.31111607e-06, ...,
          8.65887728e-07,  1.27591832e-07,  6.00685280e-06],
        ...,
        [ 3.11789017e-06, -6.15000044e-06,  1.26706154e-05, ...,
         -1.25420165e-05,  4.80279095e-06,  5.42298221e-06],
        [ 1.50459653e-06, -8.42342706e-06,  7.66077301e-06, ...,
         -1.07366031e-05, -4.16762259e-06,  7.37164510e-06],
        [-2.84778866e-06, -3.81677637e-06,  7.80785467e-06, ...,
         -1.55558564e-06, -9.58248734e-07,  5.77966830e-06]], dtype=float32), indices=array([9970, 9971, 9972, ..., 2043,   23,    1], dtype=int32), dense_shape=array([10000,   200], dtype=int32)),
 array([[ 1.6066767e-08,  3.5067192e-08, -1.1746049e-10, ...,
          5.8436541e-08, -2.3235570e-08, -3.1973450e-08],
        [-5.5296660e-08, -3.3499763e-08,  3.5328263e-08, ...,
         -2.5721681e-08,  2.9637341e-08,  4.8090200e-08],
        [-8.4972394e-09, -2.5726406e-08,  6.3189923e-08, ...,
         -2.8150120e-08, -1.6127135e-09,  4.9952789e-08],
        ...,
        [ 5.2933906e-09, -2.5581681e-09,  2.5126173e-10, ...,
          1.1959569e-08, -6.4872872e-09,  1.1115321e-08],
        [-2.3990161e-09,  5.1252247e-10, -2.2163025e-09, ...,
         -4.3951234e-09, -2.0027489e-08, -5.1065313e-10],
        [-6.4774719e-09, -3.9391477e-09,  4.1060377e-09, ...,
          4.9047104e-09, -5.1072515e-09,  1.4783470e-08]], dtype=float32),
 array([ 1.1906226e-06,  1.2429670e-06, -3.3754725e-07, ...,
         3.4770521e-06,  2.6483815e-06,  1.4185525e-06], dtype=float32),
 array([[ 2.82152257e-09,  2.86482749e-09, -3.88555987e-09, ...,
         -4.84896434e-09,  1.49613233e-09, -3.22891602e-09],
        [ 3.33813843e-09,  8.23170598e-09,  4.96051289e-09, ...,
         -7.32058947e-09, -7.41958850e-09,  3.94051280e-09],
        [-1.12334786e-09,  3.23000893e-09,  1.58423274e-09, ...,
         -2.84767854e-09, -1.09925735e-10, -2.44763632e-09],
        ...,
        [-7.39758144e-10,  3.41527673e-09, -1.07006737e-09, ...,
         -5.25751942e-10, -1.61392688e-09,  2.94533647e-10],
        [ 1.31301436e-09, -2.85678481e-10, -6.59250876e-10, ...,
         -8.46696158e-10, -7.34119121e-09, -1.56427304e-09],
        [-1.62541258e-10,  9.05795772e-10,  2.15344617e-10, ...,
         -2.96144687e-09, -3.10188675e-09,  9.89087834e-09]], dtype=float32),
 array([ 4.69054112e-06,  4.68102058e-07, -9.92032938e-07,  5.60780336e-07,
        -2.59410217e-06, -2.22886888e-06,  2.45557612e-06, -8.80732955e-07,
        -4.70148223e-08,  4.76234709e-06, -3.70097268e-07, -2.24516452e-06,
        -3.53951759e-06,  7.30050715e-07,  3.52373775e-07,  2.82506198e-06,
         1.71158308e-06,  2.37686118e-06, -1.92323273e-06, -2.41163934e-06,
         1.60618094e-06,  9.53752647e-07,  4.75565230e-06,  5.22564619e-07,
         3.44931459e-06,  4.31589683e-07,  6.04613479e-06, -2.25653025e-06,
         6.04737852e-06, -8.20218611e-06,  1.56185456e-06,  3.34413653e-06,
         1.52187476e-06, -7.72086150e-06,  2.58739783e-06, -9.71262352e-07,
         7.35883532e-06,  1.55628629e-06, -3.24571907e-08, -3.44389582e-06,
         2.01040643e-06, -9.10827566e-07,  1.17336839e-07,  8.35887658e-06,
        -2.45897195e-06, -2.53624103e-06,  2.34787217e-06, -2.08731808e-06,
         1.73975025e-07,  7.02792431e-06,  5.80364303e-06, -8.19647448e-06,
        -4.06862142e-07,  2.30448222e-06, -3.65284473e-06,  7.44432145e-06,
         5.23486824e-06, -1.98140970e-06, -2.61483933e-07,  6.83807571e-07,
        -7.05898856e-06,  3.94286144e-06,  4.43415047e-07,  6.91718355e-07,
         6.51707410e-07, -6.59503826e-07,  1.59254427e-07, -5.82457278e-06,
         1.97188115e-06, -4.30781893e-06,  6.94200196e-07,  1.51332915e-06,
         1.62943354e-06, -1.78604489e-06,  4.05077515e-07,  1.47902506e-06,
        -4.87039188e-06, -1.63671677e-06, -4.06812296e-06,  1.08095878e-06,
         3.04282685e-06,  1.11368149e-06,  4.61136506e-06,  5.18808633e-07,
        -1.17497575e-06, -2.02238107e-06,  1.82892177e-06,  1.90911760e-06,
        -3.12053521e-06, -3.55679867e-06, -2.95036489e-06,  6.15979798e-06,
         5.11595317e-08,  1.79633309e-06,  8.06270066e-07,  1.15697208e-06,
        -2.28780527e-06,  1.64003427e-06, -6.33194929e-07,  6.09375866e-06,
        -6.34333583e-06,  3.52051103e-07, -5.96862446e-06, -1.47543915e-06,
        -7.09807682e-06, -9.97329153e-07,  3.48767890e-06,  1.11682402e-06,
        -5.72341469e-06,  4.75494153e-06,  5.31957357e-06, -1.00912462e-06,
        -4.58540671e-06, -9.97827556e-07,  2.29915736e-06, -1.90672267e-06,
         4.84219072e-07, -9.30472538e-07,  4.36010032e-06,  5.73212947e-06,
         1.12038651e-06, -6.01142756e-06, -1.80933591e-06,  8.50413358e-07,
        -3.40388624e-06, -4.88233718e-06, -2.65788958e-06, -4.23499250e-06,
        -1.67782698e-02,  1.40324084e-03, -2.05439292e-02, -1.56230200e-02,
        -6.46257307e-03,  2.13513356e-02,  2.61986221e-04, -6.80318754e-03,
         8.11728928e-03, -2.36294698e-02,  1.34925721e-02, -6.54587476e-03,
        -2.40131449e-02, -2.80507677e-03, -1.14419786e-02, -2.15098262e-02,
         6.07113261e-03, -8.78779124e-03, -1.12909470e-02, -1.30228950e-02,
         6.79672044e-03, -7.87025201e-04, -1.54209752e-02, -1.13330770e-03,
         6.75834669e-03, -2.03032754e-02, -1.71342976e-02, -1.70419086e-02,
        -6.20212522e-04, -4.47902940e-02, -1.23471338e-02, -1.36425393e-02,
         2.72955038e-02,  1.62695069e-02,  2.00330019e-02, -1.94328446e-02,
         1.48442900e-02,  7.48993829e-03,  1.59891397e-02,  1.93818249e-02,
         1.45618357e-02, -4.68219118e-03, -1.07645923e-02, -2.57405415e-02,
        -1.12656774e-02,  5.06304624e-03,  2.43847836e-02,  2.09896471e-02,
        -3.65569303e-03,  3.23981717e-02, -2.11831108e-02, -2.16212459e-02,
         1.83932623e-03, -9.95439477e-03,  1.38720982e-02,  3.85212749e-02,
        -1.83281209e-03,  2.29631439e-02, -6.64313324e-03,  7.27478997e-04,
        -2.25201733e-02,  2.22989451e-02,  2.39708787e-03, -5.05504990e-03,
         8.49392451e-03,  8.86597577e-03,  1.13722973e-03,  1.74824353e-02,
         1.91654935e-02, -1.33035919e-02,  1.20981587e-02,  9.90184862e-03,
        -1.21057807e-02,  6.36008568e-03, -9.03539546e-03,  1.12846782e-02,
         1.09304842e-02,  1.09546296e-02, -6.41993899e-03,  1.09641282e-02,
         2.13901065e-02,  1.60594180e-04, -1.46217626e-02,  4.13014274e-03,
         1.29602570e-02,  1.54162208e-02,  1.77017748e-02,  2.54471600e-02,
        -2.89960224e-02, -2.29745246e-02, -5.75283915e-03,  2.03590151e-02,
         3.25137414e-02, -1.80340596e-02,  1.40880691e-02,  3.62778665e-03,
        -5.94495470e-03, -2.83740670e-03,  1.78599823e-02, -1.38584413e-02,
         1.76584721e-02, -7.64559535e-03,  2.13941969e-02, -2.89646275e-02,
        -6.42309431e-03, -1.53222857e-02,  1.37541126e-02,  6.65195240e-03,
        -2.51375921e-02, -8.90096184e-03, -3.81129645e-02, -1.56557150e-02,
        -2.47294698e-02,  1.97428744e-03,  4.07801569e-03,  1.50190741e-02,
         6.92302128e-03,  2.18006894e-02,  8.67008790e-03,  1.23183271e-02,
         1.08832121e-02, -3.18964571e-02,  3.39374598e-03, -8.71196575e-03,
         1.36874262e-02,  3.04737571e-03, -2.95267683e-02,  3.68994884e-02,
         5.75129548e-07, -2.63159109e-07,  1.30382750e-06, -9.28824534e-07,
        -3.41057671e-06,  2.60241961e-07,  1.64374831e-06, -1.34782113e-06,
        -1.17480141e-07,  3.49939455e-06, -4.55303962e-07, -5.16705722e-06,
        -4.99210728e-06,  2.11140582e-06,  8.88680688e-07,  3.36226231e-06,
         1.55396435e-06,  3.71765282e-06, -1.67785709e-06, -2.65824474e-06,
         2.59022158e-07, -3.47943842e-06,  4.32638262e-06,  4.10687733e-07,
         2.34934123e-06,  1.37174493e-06,  5.59340742e-06, -4.85302007e-06,
         4.22242874e-06, -6.43024032e-06, -8.51564891e-07,  3.74586648e-06,
         2.31881899e-07, -8.15783642e-06,  3.52595930e-06, -2.28803833e-06,
         3.69596364e-06,  7.36391939e-07, -2.94863924e-07, -4.43130421e-06,
        -3.57236013e-06, -1.50326787e-06,  3.69040663e-07,  6.46886019e-06,
        -1.46349834e-07,  3.83327887e-07,  1.44465071e-06, -1.35943628e-06,
        -2.25721536e-07,  5.16130012e-06,  4.72013789e-06, -8.52984613e-06,
        -3.62786665e-07,  2.52926520e-06, -2.52609971e-06,  3.78092000e-06,
         1.33412561e-06, -3.63973595e-06, -2.62799290e-06, -1.17601621e-06,
        -4.63347988e-06, -8.29023350e-07,  1.44919602e-06,  1.68958547e-06,
         1.84339240e-06, -7.98195344e-07,  6.55083738e-08, -4.06115578e-06,
         1.74780996e-06, -3.86603961e-06, -1.16461976e-07, -2.94971869e-07,
         3.06160246e-06, -2.27214878e-06, -3.80240590e-06,  3.58638931e-06,
        -5.06503420e-06, -6.37957237e-06, -1.63210848e-06,  1.56711292e-06,
         9.60996886e-07, -2.59937087e-06,  4.37669996e-06,  1.65900587e-06,
        -1.83292957e-06, -2.03104423e-06,  6.09795791e-07, -7.88546799e-07,
         3.92503807e-11, -3.40877523e-06, -2.22923563e-06,  5.14046133e-06,
        -1.46384551e-07,  6.95680569e-07, -3.55125081e-07,  2.48304772e-08,
        -7.78362676e-07,  8.15811120e-07,  2.60363322e-06,  5.00697570e-06,
        -5.69264103e-06, -3.05187609e-06, -4.34491130e-06,  2.75907291e-06,
        -4.87445868e-06,  1.36171684e-06,  1.29883415e-06,  5.83035444e-07,
        -4.74623994e-06,  5.28778082e-06,  8.77900584e-06, -1.58754119e-06,
         6.65091534e-07,  6.23058497e-07,  1.34108427e-06, -3.54326380e-06,
        -1.36215522e-06, -2.73867363e-06,  4.22087123e-06,  7.85362317e-06,
        -9.16213821e-07, -4.36956861e-06, -3.91406019e-07, -2.39307838e-07,
        -2.05247170e-06, -1.72826014e-06, -2.32609659e-06, -3.46989736e-06,
         4.69911993e-06,  4.65100896e-07, -9.89996124e-07,  5.53963957e-07,
        -2.58802129e-06, -2.22813560e-06,  2.46172658e-06, -8.85761324e-07,
        -4.38511449e-08,  4.76039850e-06, -3.69381894e-07, -2.23981351e-06,
        -3.53272708e-06,  7.29258829e-07,  3.57510714e-07,  2.82647807e-06,
         1.71171621e-06,  2.37360723e-06, -1.92001835e-06, -2.41163229e-06,
         1.60607181e-06,  9.57939278e-07,  4.75295792e-06,  5.15477552e-07,
         3.45039143e-06,  4.33367518e-07,  6.03808530e-06, -2.25976964e-06,
         6.03929902e-06, -8.19775687e-06,  1.55941780e-06,  3.33800085e-06,
         1.51998393e-06, -7.72514159e-06,  2.58542855e-06, -9.77377795e-07,
         7.36141328e-06,  1.55773660e-06, -3.70788200e-08, -3.44189243e-06,
         2.00991326e-06, -9.10174549e-07,  1.16632265e-07,  8.35962419e-06,
        -2.46242143e-06, -2.53434609e-06,  2.35014431e-06, -2.08532128e-06,
         1.71112504e-07,  7.02001034e-06,  5.80326923e-06, -8.19709294e-06,
        -4.08654643e-07,  2.29968919e-06, -3.65223514e-06,  7.44271892e-06,
         5.22901246e-06, -1.98165299e-06, -2.56819476e-07,  6.84374527e-07,
        -7.05250568e-06,  3.93815253e-06,  4.43990729e-07,  6.89872991e-07,
         6.53909751e-07, -6.59104558e-07,  1.69633154e-07, -5.82187067e-06,
         1.97146187e-06, -4.31157787e-06,  6.94333266e-07,  1.51266477e-06,
         1.63146103e-06, -1.78288110e-06,  4.05664395e-07,  1.47696278e-06,
        -4.86814497e-06, -1.63414097e-06, -4.06505706e-06,  1.07520077e-06,
         3.04547734e-06,  1.11393342e-06,  4.61753280e-06,  5.20912920e-07,
        -1.17195918e-06, -2.02326532e-06,  1.82895121e-06,  1.90987339e-06,
        -3.13556939e-06, -3.55966358e-06, -2.94711640e-06,  6.15707904e-06,
         4.83761369e-08,  1.79833864e-06,  8.09227515e-07,  1.15738396e-06,
        -2.28903514e-06,  1.63878872e-06, -6.29824228e-07,  6.09275503e-06,
        -6.34225535e-06,  3.59656525e-07, -5.96742984e-06, -1.47614332e-06,
        -7.09931146e-06, -9.99413942e-07,  3.49338825e-06,  1.11041788e-06,
        -5.72565204e-06,  4.76047853e-06,  5.31512296e-06, -1.00100249e-06,
        -4.58489740e-06, -9.97561074e-07,  2.29854550e-06, -1.90547996e-06,
         4.84413590e-07, -9.27973133e-07,  4.36150003e-06,  5.73578745e-06,
         1.11430541e-06, -6.02246655e-06, -1.80911707e-06,  8.52821131e-07,
        -3.40345741e-06, -4.88256183e-06, -2.65874633e-06, -4.23911843e-06],
       dtype=float32),
 array([[-2.36724190e-05,  1.65698075e-04,  1.51683998e-04, ...,
         -1.07361188e-07, -1.05886855e-07, -1.05065588e-07],
        [-3.01451189e-04, -2.43887363e-04, -2.50373443e-04, ...,
          5.98761744e-07,  5.90605680e-07,  5.85834016e-07],
        [-1.49160687e-05, -6.38432393e-05,  1.63396890e-05, ...,
          1.96839345e-09,  1.95364969e-09,  1.93524574e-09],
        ...,
        [-6.59689031e-05,  9.05689667e-05, -8.56744009e-07, ...,
          1.45642076e-07,  1.43621051e-07,  1.42496731e-07],
        [-4.08373380e-05, -1.14960385e-04,  1.56148293e-04, ...,
         -6.80299053e-08, -6.71384015e-08, -6.65706850e-08],
        [ 1.13050970e-04,  6.82620157e-05,  1.66142563e-04, ...,
         -1.12256231e-07, -1.10693804e-07, -1.09840236e-07]], dtype=float32),
 array([-0.78133273, -1.0979931 , -0.98132986, ...,  0.00202222,
         0.00199453,  0.00197843], dtype=float32)]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>4. Apply the optimizer to the variables / gradients tuple.</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[48]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create the training TensorFlow Operation through our optimizer</span>
<span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">tvars</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[49]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a></a></p>
<h2>LSTM</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We learned how the model is build step by step. Noe, let's then create a Class that represents our model. This class needs a few things:</p>
<ul>
    <li>We have to create the model in accordance with our defined hyperparameters</li>
    <li>We have to create the placeholders for our input data and expected outputs (the real data)</li>
    <li>We have to create the LSTM cell structure and connect them with our RNN structure</li>
    <li>We have to create the word embeddings and point them to the input data</li>
    <li>We have to create the input structure for our RNN</li>
    <li>We have to instantiate our RNN model and retrieve the variable in which we should expect our outputs to appear</li>
    <li>We need to create a logistic structure to return the probability of our words</li>
    <li>We need to create the loss and cost functions for our optimizer to work, and then create the optimizer</li>
    <li>And finally, we need to create a training operation that can be run to actually train our model</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[50]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden_size_l1</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[50]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>256</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[51]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">PTBModel</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action_type</span><span class="p">):</span>
        <span class="c1">######################################</span>
        <span class="c1"># Setting parameters for ease of use #</span>
        <span class="c1">######################################</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_steps</span> <span class="o">=</span> <span class="n">num_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_l1</span> <span class="o">=</span> <span class="n">hidden_size_l1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_l2</span> <span class="o">=</span> <span class="n">hidden_size_l2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeding_vector_size</span> <span class="o">=</span> <span class="n">embeding_vector_size</span>
        <span class="c1">###############################################################################</span>
        <span class="c1"># Creating placeholders for our input data and expected outputs (target data) #</span>
        <span class="c1">###############################################################################</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">])</span> <span class="c1">#[30#20]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_targets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">])</span> <span class="c1">#[30#20]</span>

        <span class="c1">##########################################################################</span>
        <span class="c1"># Creating the LSTM cell structure and connect it with the RNN structure #</span>
        <span class="c1">##########################################################################</span>
        <span class="c1"># Create the LSTM unit. </span>
        <span class="c1"># This creates only the structure for the LSTM and has to be associated with a RNN unit still.</span>
        <span class="c1"># The argument n_hidden(size=200) of BasicLSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A).</span>
        <span class="c1"># Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate. </span>
        <span class="c1"># LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.</span>
        <span class="n">lstm_cell_l1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_l1</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="n">lstm_cell_l2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_l2</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        
        <span class="c1"># Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit</span>
        <span class="c1"># This is an optimization of the LSTM output, but is not needed at all</span>
        <span class="k">if</span> <span class="n">action_type</span> <span class="o">==</span> <span class="s2">"is_training"</span> <span class="ow">and</span> <span class="n">keep_prob</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">lstm_cell_l1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">lstm_cell_l1</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">keep_prob</span><span class="p">)</span>
            <span class="n">lstm_cell_l2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">lstm_cell_l2</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">keep_prob</span><span class="p">)</span>
        
        <span class="c1"># By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units.</span>
        <span class="c1"># RNN cell composed sequentially of multiple simple cells.</span>
        <span class="n">stacked_lstm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">lstm_cell_l1</span><span class="p">,</span> <span class="n">lstm_cell_l2</span><span class="p">])</span>

        <span class="c1"># Define the initial state, i.e., the model state for the very first data point</span>
        <span class="c1"># It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initial_state</span> <span class="o">=</span> <span class="n">stacked_lstm</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1">####################################################################</span>
        <span class="c1"># Creating the word embeddings and pointing them to the input data #</span>
        <span class="c1">####################################################################</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/cpu:0"</span><span class="p">):</span>
            <span class="c1"># Create the embeddings for our input data. Size is hidden size.</span>
            <span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">,</span> <span class="p">[</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeding_vector_size</span><span class="p">])</span>  <span class="c1">#[10000x200]</span>
            <span class="c1"># Define where to get the data for our embeddings from</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_data</span><span class="p">)</span>

        <span class="c1"># Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs</span>
        <span class="c1"># This is an optimization of the input processing and is not needed at all</span>
        <span class="k">if</span> <span class="n">action_type</span> <span class="o">==</span> <span class="s2">"is_training"</span> <span class="ow">and</span> <span class="n">keep_prob</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>

        <span class="c1">############################################</span>
        <span class="c1"># Creating the input structure for our RNN #</span>
        <span class="c1">############################################</span>
        <span class="c1"># Input structure is 20x[30x200]</span>
        <span class="c1"># Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]</span>
        <span class="c1"># inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]</span>
        <span class="c1"># The input structure is fed from the embeddings, which are filled in by the input data</span>
        <span class="c1"># Feeding a batch of b sentences to a RNN:</span>
        <span class="c1"># In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  </span>
        <span class="c1"># In step 2,  second word of each of the b sentences is input in parallel. </span>
        <span class="c1"># The parallelism is only for efficiency.  </span>
        <span class="c1"># Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. </span>
        <span class="c1"># All the computations involving the words of all sentences in a batch at a given time step are done in parallel. </span>

        <span class="c1">####################################################################################################</span>
        <span class="c1"># Instantiating our RNN model and retrieving the structure for returning the outputs and the state #</span>
        <span class="c1">####################################################################################################</span>
        
        <span class="n">outputs</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">stacked_lstm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initial_state</span><span class="p">)</span>
        <span class="c1">#########################################################################</span>
        <span class="c1"># Creating a logistic unit to return the probability of the output word #</span>
        <span class="c1">#########################################################################</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_l2</span><span class="p">])</span>
        <span class="n">softmax_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">"softmax_w"</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_l2</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">])</span> <span class="c1">#[200x1000]</span>
        <span class="n">softmax_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">"softmax_b"</span><span class="p">,</span> <span class="p">[</span><span class="n">vocab_size</span><span class="p">])</span> <span class="c1">#[1x1000]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">softmax_w</span><span class="p">)</span> <span class="o">+</span> <span class="n">softmax_b</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_steps</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">])</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">out_words</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_words</span> <span class="o">=</span> <span class="n">out_words</span>
        <span class="c1">#########################################################################</span>
        <span class="c1"># Defining the loss and cost functions for the model's learning to work #</span>
        <span class="c1">#########################################################################</span>
            

        <span class="c1"># Use the contrib sequence loss and average over the batches</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">seq2seq</span><span class="o">.</span><span class="n">sequence_loss</span><span class="p">(</span>
            <span class="n">logits</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">targets</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
            <span class="n">average_across_timesteps</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">average_across_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
<span class="c1">#         loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],</span>
<span class="c1">#                                                       [tf.ones([batch_size * num_steps])])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="c1"># Store the final state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_final_state</span> <span class="o">=</span> <span class="n">state</span>

        <span class="c1">#Everything after this point is relevant only for training</span>
        <span class="k">if</span> <span class="n">action_type</span> <span class="o">!=</span> <span class="s2">"is_training"</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1">#################################################</span>
        <span class="c1"># Creating the Training Operation for our Model #</span>
        <span class="c1">#################################################</span>
        <span class="c1"># Create a variable for the learning rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lr</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Get all TensorFlow variables marked as "trainable" (i.e. all of them except _lr, which we just created)</span>
        <span class="n">tvars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()</span>
        <span class="c1"># Define the gradient clipping threshold</span>
        <span class="n">grads</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">,</span> <span class="n">tvars</span><span class="p">),</span> <span class="n">max_grad_norm</span><span class="p">)</span>
        <span class="c1"># Create the gradient descent optimizer with our learning rate</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
        <span class="c1"># Create the training TensorFlow Operation through our optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">tvars</span><span class="p">))</span>

    <span class="c1"># Helper functions for our LSTM RNN class</span>

    <span class="c1"># Assign the learning rate for this model</span>
    <span class="k">def</span> <span class="nf">assign_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">session</span><span class="p">,</span> <span class="n">lr_value</span><span class="p">):</span>
        <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">lr_value</span><span class="p">))</span>

    <span class="c1"># Returns the input data for this model at a point in time</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">input_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_data</span>


    
    <span class="c1"># Returns the targets for this model at a point in time</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">targets</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_targets</span>

    <span class="c1"># Returns the initial state for this model</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">initial_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_state</span>

    <span class="c1"># Returns the defined Cost</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span>

    <span class="c1"># Returns the final state for this model</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">final_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_final_state</span>
    
    <span class="c1"># Returns the final output words for this model</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">final_output_words</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_words</span>
    
    <span class="c1"># Returns the current learning rate for this model</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lr</span>

    <span class="c1"># Returns the training operation defined for this model</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">train_op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_op</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to actually create the methods to run through time -- that is, the <code>run_epoch</code> method to be run at each epoch and a <code>main</code> script which ties all of this together.</p>
<p>What our <code>run_epoch</code> method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[52]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">##########################################################################################################################</span>
<span class="c1"># run_one_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #</span>
<span class="c1">##########################################################################################################################</span>
<span class="k">def</span> <span class="nf">run_one_epoch</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">eval_op</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

    <span class="c1">#Define the epoch size based on the length of the data, batch size and the number of steps</span>
    <span class="n">epoch_size</span> <span class="o">=</span> <span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">//</span> <span class="n">m</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">m</span><span class="o">.</span><span class="n">num_steps</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">initial_state</span><span class="p">)</span>
    
    <span class="c1">#For each step and data point</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reader</span><span class="o">.</span><span class="n">ptb_iterator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">num_steps</span><span class="p">)):</span>
        
        <span class="c1">#Evaluate and return cost, state by running cost, final_state and the function passed as parameter</span>
        <span class="n">cost</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">out_words</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">cost</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">final_state</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">final_output_words</span><span class="p">,</span> <span class="n">eval_op</span><span class="p">],</span>
                                     <span class="p">{</span><span class="n">m</span><span class="o">.</span><span class="n">input_data</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
                                      <span class="n">m</span><span class="o">.</span><span class="n">targets</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
                                      <span class="n">m</span><span class="o">.</span><span class="n">initial_state</span><span class="p">:</span> <span class="n">state</span><span class="p">})</span>

        <span class="c1">#Add returned cost to costs (which keeps track of the total costs for this epoch)</span>
        <span class="n">costs</span> <span class="o">+=</span> <span class="n">cost</span>
        
        <span class="c1">#Add number of steps to iteration counter</span>
        <span class="n">iters</span> <span class="o">+=</span> <span class="n">m</span><span class="o">.</span><span class="n">num_steps</span>

        <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">step</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch_size</span> <span class="o">//</span> <span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Itr </span><span class="si">%d</span><span class="s2"> of </span><span class="si">%d</span><span class="s2">, perplexity: </span><span class="si">%.3f</span><span class="s2"> speed: </span><span class="si">%.0f</span><span class="s2"> wps"</span> <span class="o">%</span> <span class="p">(</span><span class="n">step</span> <span class="p">,</span> <span class="n">epoch_size</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">costs</span> <span class="o">/</span> <span class="n">iters</span><span class="p">),</span> <span class="n">iters</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">/</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)))</span>

    <span class="c1"># Returns the Perplexity rating for us to keep track of how the model is evolving</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">costs</span> <span class="o">/</span> <span class="n">iters</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, we create the <code>main</code> method to tie everything together. The code here reads the data from the directory, using the <code>reader</code> helper module, and then trains and evaluates the model on both a testing and a validating subset of data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[53]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Reads the data and separates it into training data, validation data and testing data</span>
<span class="n">raw_data</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">ptb_raw_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">valid_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">raw_data</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[54]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initializes the Execution Graph and the Session</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">(),</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
    <span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span><span class="o">-</span><span class="n">init_scale</span><span class="p">,</span> <span class="n">init_scale</span><span class="p">)</span>
    
    <span class="c1"># Instantiates the model for training</span>
    <span class="c1"># tf.variable_scope add a prefix to the variables created with tf.get_variable</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">"model"</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">PTBModel</span><span class="p">(</span><span class="s2">"is_training"</span><span class="p">)</span>
        
    <span class="c1"># Reuses the trained parameters for the validation and testing models</span>
    <span class="c1"># They are different instances but use the same variables for weights and biases, they just don't change when data is input</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">"model"</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">):</span>
        <span class="n">mvalid</span> <span class="o">=</span> <span class="n">PTBModel</span><span class="p">(</span><span class="s2">"is_validating"</span><span class="p">)</span>
        <span class="n">mtest</span> <span class="o">=</span> <span class="n">PTBModel</span><span class="p">(</span><span class="s2">"is_testing"</span><span class="p">)</span>

    <span class="c1">#Initialize all variables</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_epoch</span><span class="p">):</span>
        <span class="c1"># Define the decay for this epoch</span>
        <span class="n">lr_decay</span> <span class="o">=</span> <span class="n">decay</span> <span class="o">**</span> <span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">max_epoch_decay_lr</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
        
        <span class="c1"># Set the decayed learning rate as the learning rate for this epoch</span>
        <span class="n">m</span><span class="o">.</span><span class="n">assign_lr</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">lr_decay</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch </span><span class="si">%d</span><span class="s2"> : Learning rate: </span><span class="si">%.3f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">lr</span><span class="p">)))</span>
        
        <span class="c1"># Run the loop for this epoch in the training model</span>
        <span class="n">train_perplexity</span> <span class="o">=</span> <span class="n">run_one_epoch</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch </span><span class="si">%d</span><span class="s2"> : Train Perplexity: </span><span class="si">%.3f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">train_perplexity</span><span class="p">))</span>
        
        <span class="c1"># Run the loop for this epoch in the validation model</span>
        <span class="n">valid_perplexity</span> <span class="o">=</span> <span class="n">run_one_epoch</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">mvalid</span><span class="p">,</span> <span class="n">valid_data</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch </span><span class="si">%d</span><span class="s2"> : Valid Perplexity: </span><span class="si">%.3f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">valid_perplexity</span><span class="p">))</span>
    
    <span class="c1"># Run the loop in the testing model to see how effective was our training</span>
    <span class="n">test_perplexity</span> <span class="o">=</span> <span class="n">run_one_epoch</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">mtest</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">())</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Test Perplexity: </span><span class="si">%.3f</span><span class="s2">"</span> <span class="o">%</span> <span class="n">test_perplexity</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 1 : Learning rate: 1.000
Itr 10 of 774, perplexity: 4090.238 speed: 2260 wps
Itr 87 of 774, perplexity: 1273.646 speed: 2310 wps
Itr 164 of 774, perplexity: 982.987 speed: 2307 wps
Itr 241 of 774, perplexity: 819.627 speed: 2285 wps
Itr 318 of 774, perplexity: 724.567 speed: 2282 wps
Itr 395 of 774, perplexity: 649.213 speed: 2289 wps
Itr 472 of 774, perplexity: 587.359 speed: 2298 wps
Itr 549 of 774, perplexity: 533.067 speed: 2299 wps
Itr 626 of 774, perplexity: 489.454 speed: 2304 wps
Itr 703 of 774, perplexity: 455.080 speed: 2306 wps
Epoch 1 : Train Perplexity: 430.634
Epoch 1 : Valid Perplexity: 256.441
Epoch 2 : Learning rate: 1.000
Itr 10 of 774, perplexity: 275.627 speed: 2189 wps
Itr 87 of 774, perplexity: 238.904 speed: 2303 wps
Itr 164 of 774, perplexity: 228.938 speed: 2312 wps
Itr 241 of 774, perplexity: 219.712 speed: 2317 wps
Itr 318 of 774, perplexity: 217.133 speed: 2315 wps
Itr 395 of 774, perplexity: 211.191 speed: 2318 wps
Itr 472 of 774, perplexity: 206.899 speed: 2317 wps
Itr 549 of 774, perplexity: 200.413 speed: 2318 wps
Itr 626 of 774, perplexity: 194.814 speed: 2319 wps
Itr 703 of 774, perplexity: 190.702 speed: 2319 wps
Epoch 2 : Train Perplexity: 187.875
Epoch 2 : Valid Perplexity: 179.330
Epoch 3 : Learning rate: 1.000
Itr 10 of 774, perplexity: 188.755 speed: 2298 wps
Itr 87 of 774, perplexity: 161.439 speed: 2330 wps
Itr 164 of 774, perplexity: 157.784 speed: 2318 wps
Itr 241 of 774, perplexity: 152.883 speed: 2323 wps
Itr 318 of 774, perplexity: 152.964 speed: 2319 wps
Itr 395 of 774, perplexity: 150.248 speed: 2320 wps
Itr 472 of 774, perplexity: 148.659 speed: 2321 wps
Itr 549 of 774, perplexity: 144.914 speed: 2319 wps
Itr 626 of 774, perplexity: 142.057 speed: 2313 wps
Itr 703 of 774, perplexity: 140.223 speed: 2311 wps
Epoch 3 : Train Perplexity: 139.132
Epoch 3 : Valid Perplexity: 152.885
Epoch 4 : Learning rate: 1.000
Itr 10 of 774, perplexity: 150.212 speed: 2298 wps
Itr 87 of 774, perplexity: 128.579 speed: 2274 wps
Itr 164 of 774, perplexity: 126.476 speed: 2281 wps
Itr 241 of 774, perplexity: 123.077 speed: 2296 wps
Itr 318 of 774, perplexity: 123.639 speed: 2303 wps
Itr 395 of 774, perplexity: 121.808 speed: 2308 wps
Itr 472 of 774, perplexity: 120.991 speed: 2309 wps
Itr 549 of 774, perplexity: 118.125 speed: 2308 wps
Itr 626 of 774, perplexity: 116.220 speed: 2312 wps
Itr 703 of 774, perplexity: 115.105 speed: 2314 wps
Epoch 4 : Train Perplexity: 114.575
Epoch 4 : Valid Perplexity: 140.927
Epoch 5 : Learning rate: 1.000
Itr 10 of 774, perplexity: 127.475 speed: 2183 wps
Itr 87 of 774, perplexity: 108.904 speed: 2297 wps
Itr 164 of 774, perplexity: 107.815 speed: 2310 wps
Itr 241 of 774, perplexity: 105.222 speed: 2309 wps
Itr 318 of 774, perplexity: 105.946 speed: 2312 wps
Itr 395 of 774, perplexity: 104.478 speed: 2313 wps
Itr 472 of 774, perplexity: 104.049 speed: 2311 wps
Itr 549 of 774, perplexity: 101.734 speed: 2312 wps
Itr 626 of 774, perplexity: 100.279 speed: 2313 wps
Itr 703 of 774, perplexity: 99.505 speed: 2314 wps
Epoch 5 : Train Perplexity: 99.242
Epoch 5 : Valid Perplexity: 136.334
Epoch 6 : Learning rate: 0.500
Itr 10 of 774, perplexity: 109.463 speed: 2300 wps
Itr 87 of 774, perplexity: 93.078 speed: 2295 wps
Itr 164 of 774, perplexity: 91.362 speed: 2305 wps
Itr 241 of 774, perplexity: 88.410 speed: 2304 wps
Itr 318 of 774, perplexity: 88.570 speed: 2307 wps
Itr 395 of 774, perplexity: 86.818 speed: 2306 wps
Itr 472 of 774, perplexity: 86.060 speed: 2309 wps
Itr 549 of 774, perplexity: 83.664 speed: 2310 wps
Itr 626 of 774, perplexity: 82.051 speed: 2309 wps
Itr 703 of 774, perplexity: 81.016 speed: 2310 wps
Epoch 6 : Train Perplexity: 80.469
Epoch 6 : Valid Perplexity: 126.533
Epoch 7 : Learning rate: 0.250
Itr 10 of 774, perplexity: 95.412 speed: 2285 wps
Itr 87 of 774, perplexity: 82.098 speed: 2303 wps
Itr 164 of 774, perplexity: 80.722 speed: 2304 wps
Itr 241 of 774, perplexity: 78.096 speed: 2286 wps
Itr 318 of 774, perplexity: 78.268 speed: 2288 wps
Itr 395 of 774, perplexity: 76.613 speed: 2286 wps
Itr 472 of 774, perplexity: 75.848 speed: 2288 wps
Itr 549 of 774, perplexity: 73.613 speed: 2291 wps
Itr 626 of 774, perplexity: 72.072 speed: 2291 wps
Itr 703 of 774, perplexity: 71.011 speed: 2291 wps
Epoch 7 : Train Perplexity: 70.403
Epoch 7 : Valid Perplexity: 124.164
Epoch 8 : Learning rate: 0.125
Itr 10 of 774, perplexity: 87.555 speed: 2251 wps
Itr 87 of 774, perplexity: 75.838 speed: 2256 wps
Itr 164 of 774, perplexity: 74.743 speed: 2262 wps
Itr 241 of 774, perplexity: 72.326 speed: 2273 wps
Itr 318 of 774, perplexity: 72.535 speed: 2278 wps
Itr 395 of 774, perplexity: 70.990 speed: 2283 wps
Itr 472 of 774, perplexity: 70.259 speed: 2287 wps
Itr 549 of 774, perplexity: 68.131 speed: 2289 wps
Itr 626 of 774, perplexity: 66.643 speed: 2288 wps
Itr 703 of 774, perplexity: 65.588 speed: 2288 wps
Epoch 8 : Train Perplexity: 64.973
Epoch 8 : Valid Perplexity: 123.072
Epoch 9 : Learning rate: 0.062
Itr 10 of 774, perplexity: 83.153 speed: 2176 wps
Itr 87 of 774, perplexity: 72.360 speed: 2258 wps
Itr 164 of 774, perplexity: 71.396 speed: 2270 wps
Itr 241 of 774, perplexity: 69.130 speed: 2285 wps
Itr 318 of 774, perplexity: 69.361 speed: 2293 wps
Itr 395 of 774, perplexity: 67.884 speed: 2297 wps
Itr 472 of 774, perplexity: 67.180 speed: 2301 wps
Itr 549 of 774, perplexity: 65.124 speed: 2303 wps
Itr 626 of 774, perplexity: 63.673 speed: 2301 wps
Itr 703 of 774, perplexity: 62.632 speed: 2305 wps
Epoch 9 : Train Perplexity: 62.023
Epoch 9 : Valid Perplexity: 122.462
Epoch 10 : Learning rate: 0.031
Itr 10 of 774, perplexity: 80.891 speed: 2211 wps
Itr 87 of 774, perplexity: 70.499 speed: 2284 wps
Itr 164 of 774, perplexity: 69.555 speed: 2295 wps
Itr 241 of 774, perplexity: 67.382 speed: 2290 wps
Itr 318 of 774, perplexity: 67.617 speed: 2289 wps
Itr 395 of 774, perplexity: 66.175 speed: 2293 wps
Itr 472 of 774, perplexity: 65.481 speed: 2294 wps
Itr 549 of 774, perplexity: 63.473 speed: 2296 wps
Itr 626 of 774, perplexity: 62.043 speed: 2294 wps
Itr 703 of 774, perplexity: 61.019 speed: 2297 wps
Epoch 10 : Train Perplexity: 60.417
Epoch 10 : Valid Perplexity: 122.073
Epoch 11 : Learning rate: 0.016
Itr 10 of 774, perplexity: 79.714 speed: 2288 wps
Itr 87 of 774, perplexity: 69.459 speed: 2276 wps
Itr 164 of 774, perplexity: 68.535 speed: 2274 wps
Itr 241 of 774, perplexity: 66.402 speed: 2274 wps
Itr 318 of 774, perplexity: 66.637 speed: 2289 wps
Itr 395 of 774, perplexity: 65.223 speed: 2295 wps
Itr 472 of 774, perplexity: 64.534 speed: 2299 wps
Itr 549 of 774, perplexity: 62.555 speed: 2302 wps
Itr 626 of 774, perplexity: 61.140 speed: 2299 wps
Itr 703 of 774, perplexity: 60.128 speed: 2299 wps
Epoch 11 : Train Perplexity: 59.533
Epoch 11 : Valid Perplexity: 121.913
Epoch 12 : Learning rate: 0.008
Itr 10 of 774, perplexity: 79.105 speed: 2242 wps
Itr 87 of 774, perplexity: 68.870 speed: 2315 wps
Itr 164 of 774, perplexity: 67.972 speed: 2315 wps
Itr 241 of 774, perplexity: 65.861 speed: 2320 wps
Itr 318 of 774, perplexity: 66.092 speed: 2316 wps
Itr 395 of 774, perplexity: 64.697 speed: 2310 wps
Itr 472 of 774, perplexity: 64.013 speed: 2313 wps
Itr 549 of 774, perplexity: 62.051 speed: 2311 wps
Itr 626 of 774, perplexity: 60.643 speed: 2311 wps
Itr 703 of 774, perplexity: 59.637 speed: 2314 wps
Epoch 12 : Train Perplexity: 59.045
Epoch 12 : Valid Perplexity: 121.826
Epoch 13 : Learning rate: 0.004
Itr 10 of 774, perplexity: 78.787 speed: 2279 wps
Itr 87 of 774, perplexity: 68.557 speed: 2306 wps
Itr 164 of 774, perplexity: 67.671 speed: 2302 wps
Itr 241 of 774, perplexity: 65.576 speed: 2308 wps
Itr 318 of 774, perplexity: 65.805 speed: 2303 wps
Itr 395 of 774, perplexity: 64.415 speed: 2309 wps
Itr 472 of 774, perplexity: 63.735 speed: 2306 wps
Itr 549 of 774, perplexity: 61.783 speed: 2298 wps
Itr 626 of 774, perplexity: 60.378 speed: 2299 wps
Itr 703 of 774, perplexity: 59.374 speed: 2294 wps
Epoch 13 : Train Perplexity: 58.785
Epoch 13 : Valid Perplexity: 121.706
Epoch 14 : Learning rate: 0.002
Itr 10 of 774, perplexity: 78.599 speed: 2224 wps
Itr 87 of 774, perplexity: 68.389 speed: 2288 wps
Itr 164 of 774, perplexity: 67.510 speed: 2296 wps
Itr 241 of 774, perplexity: 65.426 speed: 2297 wps
Itr 318 of 774, perplexity: 65.657 speed: 2296 wps
Itr 395 of 774, perplexity: 64.269 speed: 2297 wps
Itr 472 of 774, perplexity: 63.590 speed: 2299 wps
Itr 549 of 774, perplexity: 61.643 speed: 2301 wps
Itr 626 of 774, perplexity: 60.240 speed: 2298 wps
Itr 703 of 774, perplexity: 59.238 speed: 2299 wps
Epoch 14 : Train Perplexity: 58.649
Epoch 14 : Valid Perplexity: 121.569
Epoch 15 : Learning rate: 0.001
Itr 10 of 774, perplexity: 78.476 speed: 2286 wps
Itr 87 of 774, perplexity: 68.285 speed: 2290 wps
Itr 164 of 774, perplexity: 67.414 speed: 2300 wps
Itr 241 of 774, perplexity: 65.338 speed: 2300 wps
Itr 318 of 774, perplexity: 65.573 speed: 2300 wps
Itr 395 of 774, perplexity: 64.188 speed: 2299 wps
Itr 472 of 774, perplexity: 63.511 speed: 2301 wps
Itr 549 of 774, perplexity: 61.567 speed: 2303 wps
Itr 626 of 774, perplexity: 60.165 speed: 2301 wps
Itr 703 of 774, perplexity: 59.165 speed: 2296 wps
Epoch 15 : Train Perplexity: 58.577
Epoch 15 : Valid Perplexity: 121.474
Test Perplexity: 117.999
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see, the model's perplexity rating drops very quickly after a few iterations. As was elaborated before, <b>lower Perplexity means that the model is more certain about its prediction</b>. As such, we can be sure that this model is performing well!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is the end of the <b>Applying Recurrent Neural Networks to Text Processing</b> notebook. Hopefully you now have a better understanding of Recurrent Neural Networks and how to implement one utilizing TensorFlow. Thank you for reading this notebook, and good luck on your studies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Want to learn more?<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=bf1eb12b035d5fa9d6b9fe484211c3f46a8246f8&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4b6f6e75546563682f4275696c64696e672d446565702d4c6561726e696e672d4d6f64656c732d776974682d54656e736f72466c6f772f626631656231326230333564356661396436623966653438343231316333663436613832343666382f4d4c30313230454e2d332e322d5265766965772d4c53544d2d4c616e67756167654d6f64656c6c696e672e6970796e62&amp;nwo=KonuTech%2FBuilding-Deep-Learning-Models-with-TensorFlow&amp;path=ML0120EN-3.2-Review-LSTM-LanguageModelling.ipynb&amp;repository_id=233910049&amp;repository_type=Repository#Want-to-learn-more?">¶</a>
</h2>
<p>Running deep learning programs usually needs a high performance platform. <strong>PowerAI</strong> speeds up deep learning and AI. Built on IBM’s Power Systems, <strong>PowerAI</strong> is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The <strong>PowerAI</strong> platform supports popular machine learning libraries and dependencies including TensorFlow, Caffe, Torch, and Theano. You can use <a href="https://cocl.us/ML0120EN_PAI">PowerAI on IMB Cloud</a>.</p>
<p>Also, you can use <strong>Watson Studio</strong> to run these notebooks faster with bigger datasets.<strong>Watson Studio</strong> is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, <strong>Watson Studio</strong> enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of <strong>Watson Studio</strong> users today with a free account at <a href="https://cocl.us/ML0120EN_DSX">Watson Studio</a>.This is the end of this lesson. Thank you for reading this notebook, and good luck on your studies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3>Thanks for completing this lesson!<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=bf1eb12b035d5fa9d6b9fe484211c3f46a8246f8&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4b6f6e75546563682f4275696c64696e672d446565702d4c6561726e696e672d4d6f64656c732d776974682d54656e736f72466c6f772f626631656231326230333564356661396436623966653438343231316333663436613832343666382f4d4c30313230454e2d332e322d5265766965772d4c53544d2d4c616e67756167654d6f64656c6c696e672e6970796e62&amp;nwo=KonuTech%2FBuilding-Deep-Learning-Models-with-TensorFlow&amp;path=ML0120EN-3.2-Review-LSTM-LanguageModelling.ipynb&amp;repository_id=233910049&amp;repository_type=Repository#Thanks-for-completing-this-lesson!">¶</a>
</h3>
<p>Notebook created by <a href="https://br.linkedin.com/in/walter-gomes-de-amorim-junior-624726121">Walter Gomes de Amorim Junior</a>, &lt;a href = "<a href="https://linkedin.com/in/saeedaghabozorgi">https://linkedin.com/in/saeedaghabozorgi"&gt;</a> Saeed Aghabozorgi &lt;/a&gt;&lt;/h4&gt;</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

<p>Copyright © 2018 <a href="https://cocl.us/DX0108EN_CC">Cognitive Class</a>. This notebook and its source code are released under the terms of the <a href="https://bigdatauniversity.com/mit-license/">MIT License</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>
</pre></div>

</div>
</div>
</div>

</div>
 

</div>

  </div>

  



</body></html>